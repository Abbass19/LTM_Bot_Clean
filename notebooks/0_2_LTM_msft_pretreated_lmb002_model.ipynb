{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import math\n",
    "import random #to set the seed to replicate results\n",
    "from datetime import datetime,timedelta #for today's date\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import sys\n",
    "\n",
    "import psycopg2\n",
    "import psycopg2.extras\n",
    "from psycopg2.extensions import AsIs\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np #for the e_logarithmic filter (and also some other mathematical operations)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler #for preprocessing, it scales features using statistics that are robust to outliers.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from scipy.stats import linregress #for the slope and the value of Y at X=0 of the linear trend line\n",
    "from scipy.optimize import curve_fit\n",
    "import tsmoothie #for the Kalman filter, it is an efficient recursive filter that evaluates the state of a dynamic system starting from a series of measurements subject to noise.\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, LSTM #the two main layers of the model\n",
    "from tensorflow.keras import optimizers #for the training of the model\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "random.seed(42)  #set the seed to replicate results\n",
    "\n",
    "print(tf.config.list_physical_devices('GPU'))\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "plt.rc(\n",
    "    'figure',\n",
    "    autolayout=True,\n",
    "    figsize=(11,4)\n",
    ")\n",
    "\n",
    "plt.rc(\n",
    "    'axes',\n",
    "    labelweight='bold',\n",
    "    labelsize='large',\n",
    "    titleweight='bold',\n",
    "    titlesize=20,\n",
    "    titlepad=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LMB Code Before Training Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Today is 20240112\n",
      "TSLA\n",
      "11013\n",
      "You are connect to the Database: dyDATA_new\n",
      "Your SQL has executed successfully\n",
      "['cleaned_raw_features_id', 'cleaned_raw_features_DCP_date_current_period', 'calculated_features_DNCP', 'cleaned_raw_features_OPCP_open_price_current_period', 'cleaned_raw_features_HPCP_high_price_current_period', 'cleaned_raw_features_LPCP_low_price_current_period', 'cleaned_raw_features_CPCP_close_price_current_period', 'cleaned_raw_features_ACPCP_adjusted_close_price_current_period', 'cleaned_raw_features_VTCP_volume_of_transactions_current_period', 'calculated_targets_MPN5P', 'calculated_targets_HPN5P', 'calculated_targets_LPN5P', 'calculated_targets_HPN1P', 'calculated_targets_LPN1P']\n",
      "Training size:  751\n",
      "Prediction size:  1014\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>DNCP</th>\n",
       "      <th>OPCP</th>\n",
       "      <th>HPCP</th>\n",
       "      <th>LPCP</th>\n",
       "      <th>CPCP</th>\n",
       "      <th>ACPCP</th>\n",
       "      <th>VTCP</th>\n",
       "      <th>OPCP_Ratio</th>\n",
       "      <th>HPCP_Ratio</th>\n",
       "      <th>LPCP_Ratio</th>\n",
       "      <th>...</th>\n",
       "      <th>MPN-4P</th>\n",
       "      <th>MPN-5P</th>\n",
       "      <th>HPN5P_Ratio</th>\n",
       "      <th>HPN5P</th>\n",
       "      <th>LPN5P_Ratio</th>\n",
       "      <th>LPN5P</th>\n",
       "      <th>hpn1p</th>\n",
       "      <th>hpn1p_Ratio</th>\n",
       "      <th>lpn1p</th>\n",
       "      <th>lpn1p_Ratio</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>43832.0</td>\n",
       "      <td>28.300000</td>\n",
       "      <td>28.713333</td>\n",
       "      <td>28.114000</td>\n",
       "      <td>28.684000</td>\n",
       "      <td>28.684000</td>\n",
       "      <td>142981500.0</td>\n",
       "      <td>0.986613</td>\n",
       "      <td>1.001023</td>\n",
       "      <td>0.980128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.159299</td>\n",
       "      <td>33.253334</td>\n",
       "      <td>1.015479</td>\n",
       "      <td>29.128000</td>\n",
       "      <td>30.266666</td>\n",
       "      <td>1.055176</td>\n",
       "      <td>29.128000</td>\n",
       "      <td>1.015479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>43833.0</td>\n",
       "      <td>29.366667</td>\n",
       "      <td>30.266666</td>\n",
       "      <td>29.128000</td>\n",
       "      <td>29.534000</td>\n",
       "      <td>29.534000</td>\n",
       "      <td>266677500.0</td>\n",
       "      <td>0.994334</td>\n",
       "      <td>1.024808</td>\n",
       "      <td>0.986253</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.125934</td>\n",
       "      <td>33.253334</td>\n",
       "      <td>0.993206</td>\n",
       "      <td>29.333332</td>\n",
       "      <td>30.104000</td>\n",
       "      <td>1.019300</td>\n",
       "      <td>29.333332</td>\n",
       "      <td>0.993206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>43836.0</td>\n",
       "      <td>29.364668</td>\n",
       "      <td>30.104000</td>\n",
       "      <td>29.333332</td>\n",
       "      <td>30.102667</td>\n",
       "      <td>30.102667</td>\n",
       "      <td>151995010.0</td>\n",
       "      <td>0.975484</td>\n",
       "      <td>1.000044</td>\n",
       "      <td>0.974443</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.164083</td>\n",
       "      <td>35.042000</td>\n",
       "      <td>1.004031</td>\n",
       "      <td>30.224000</td>\n",
       "      <td>31.442000</td>\n",
       "      <td>1.044492</td>\n",
       "      <td>30.224000</td>\n",
       "      <td>1.004031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>43837.0</td>\n",
       "      <td>30.760000</td>\n",
       "      <td>31.442000</td>\n",
       "      <td>30.224000</td>\n",
       "      <td>31.270666</td>\n",
       "      <td>31.270666</td>\n",
       "      <td>268231500.0</td>\n",
       "      <td>0.983669</td>\n",
       "      <td>1.005479</td>\n",
       "      <td>0.966529</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.167036</td>\n",
       "      <td>36.494000</td>\n",
       "      <td>0.998231</td>\n",
       "      <td>31.215334</td>\n",
       "      <td>33.232666</td>\n",
       "      <td>1.062743</td>\n",
       "      <td>31.215334</td>\n",
       "      <td>0.998231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>43838.0</td>\n",
       "      <td>31.580000</td>\n",
       "      <td>33.232666</td>\n",
       "      <td>31.215334</td>\n",
       "      <td>32.809334</td>\n",
       "      <td>32.809334</td>\n",
       "      <td>467164500.0</td>\n",
       "      <td>0.962531</td>\n",
       "      <td>1.012903</td>\n",
       "      <td>0.951416</td>\n",
       "      <td>...</td>\n",
       "      <td>30.924166</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.112305</td>\n",
       "      <td>36.494000</td>\n",
       "      <td>0.960844</td>\n",
       "      <td>31.524668</td>\n",
       "      <td>33.253334</td>\n",
       "      <td>1.013533</td>\n",
       "      <td>31.524668</td>\n",
       "      <td>0.960844</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>45296.0</td>\n",
       "      <td>236.860000</td>\n",
       "      <td>240.119600</td>\n",
       "      <td>234.900100</td>\n",
       "      <td>237.490000</td>\n",
       "      <td>237.490000</td>\n",
       "      <td>92084160.0</td>\n",
       "      <td>0.997347</td>\n",
       "      <td>1.011072</td>\n",
       "      <td>0.989095</td>\n",
       "      <td>...</td>\n",
       "      <td>239.402500</td>\n",
       "      <td>242.700000</td>\n",
       "      <td>1.015790</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>0.948966</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>1.015790</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>0.990779</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>45299.0</td>\n",
       "      <td>236.140000</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>240.450000</td>\n",
       "      <td>240.450000</td>\n",
       "      <td>84705016.0</td>\n",
       "      <td>0.982075</td>\n",
       "      <td>1.003286</td>\n",
       "      <td>0.978582</td>\n",
       "      <td>...</td>\n",
       "      <td>238.110000</td>\n",
       "      <td>239.402500</td>\n",
       "      <td>1.003286</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>0.937284</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>238.964600</td>\n",
       "      <td>0.993822</td>\n",
       "      <td>232.040000</td>\n",
       "      <td>0.965024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1011</th>\n",
       "      <td>45300.0</td>\n",
       "      <td>238.110000</td>\n",
       "      <td>238.964600</td>\n",
       "      <td>232.040000</td>\n",
       "      <td>234.960000</td>\n",
       "      <td>234.960000</td>\n",
       "      <td>96267000.0</td>\n",
       "      <td>1.013407</td>\n",
       "      <td>1.017044</td>\n",
       "      <td>0.987572</td>\n",
       "      <td>...</td>\n",
       "      <td>237.342425</td>\n",
       "      <td>238.110000</td>\n",
       "      <td>1.026728</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>0.959185</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>1.002298</td>\n",
       "      <td>231.290000</td>\n",
       "      <td>0.984380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1012</th>\n",
       "      <td>45301.0</td>\n",
       "      <td>235.100000</td>\n",
       "      <td>235.500000</td>\n",
       "      <td>231.290000</td>\n",
       "      <td>233.940000</td>\n",
       "      <td>233.940000</td>\n",
       "      <td>91483000.0</td>\n",
       "      <td>1.004959</td>\n",
       "      <td>1.006668</td>\n",
       "      <td>0.988672</td>\n",
       "      <td>...</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>237.342425</td>\n",
       "      <td>1.031205</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>0.963367</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>230.915000</td>\n",
       "      <td>0.987069</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>0.963367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1013</th>\n",
       "      <td>45302.0</td>\n",
       "      <td>230.570000</td>\n",
       "      <td>230.915000</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>227.220000</td>\n",
       "      <td>227.220000</td>\n",
       "      <td>104632210.0</td>\n",
       "      <td>1.014743</td>\n",
       "      <td>1.016262</td>\n",
       "      <td>0.991858</td>\n",
       "      <td>...</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>235.300000</td>\n",
       "      <td>1.061702</td>\n",
       "      <td>241.240000</td>\n",
       "      <td>0.991858</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>230.915000</td>\n",
       "      <td>1.016262</td>\n",
       "      <td>225.370000</td>\n",
       "      <td>0.991858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1014 rows Ã— 26 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         DNCP        OPCP        HPCP        LPCP        CPCP       ACPCP  \\\n",
       "0     43832.0   28.300000   28.713333   28.114000   28.684000   28.684000   \n",
       "1     43833.0   29.366667   30.266666   29.128000   29.534000   29.534000   \n",
       "2     43836.0   29.364668   30.104000   29.333332   30.102667   30.102667   \n",
       "3     43837.0   30.760000   31.442000   30.224000   31.270666   31.270666   \n",
       "4     43838.0   31.580000   33.232666   31.215334   32.809334   32.809334   \n",
       "...       ...         ...         ...         ...         ...         ...   \n",
       "1009  45296.0  236.860000  240.119600  234.900100  237.490000  237.490000   \n",
       "1010  45299.0  236.140000  241.240000  235.300000  240.450000  240.450000   \n",
       "1011  45300.0  238.110000  238.964600  232.040000  234.960000  234.960000   \n",
       "1012  45301.0  235.100000  235.500000  231.290000  233.940000  233.940000   \n",
       "1013  45302.0  230.570000  230.915000  225.370000  227.220000  227.220000   \n",
       "\n",
       "             VTCP  OPCP_Ratio  HPCP_Ratio  LPCP_Ratio  ...      MPN-4P  \\\n",
       "0     142981500.0    0.986613    1.001023    0.980128  ...    0.000000   \n",
       "1     266677500.0    0.994334    1.024808    0.986253  ...    0.000000   \n",
       "2     151995010.0    0.975484    1.000044    0.974443  ...    0.000000   \n",
       "3     268231500.0    0.983669    1.005479    0.966529  ...    0.000000   \n",
       "4     467164500.0    0.962531    1.012903    0.951416  ...   30.924166   \n",
       "...           ...         ...         ...         ...  ...         ...   \n",
       "1009   92084160.0    0.997347    1.011072    0.989095  ...  239.402500   \n",
       "1010   84705016.0    0.982075    1.003286    0.978582  ...  238.110000   \n",
       "1011   96267000.0    1.013407    1.017044    0.987572  ...  237.342425   \n",
       "1012   91483000.0    1.004959    1.006668    0.988672  ...  235.300000   \n",
       "1013  104632210.0    1.014743    1.016262    0.991858  ...  235.300000   \n",
       "\n",
       "          MPN-5P  HPN5P_Ratio       HPN5P  LPN5P_Ratio       LPN5P  \\\n",
       "0       0.000000     1.159299   33.253334     1.015479   29.128000   \n",
       "1       0.000000     1.125934   33.253334     0.993206   29.333332   \n",
       "2       0.000000     1.164083   35.042000     1.004031   30.224000   \n",
       "3       0.000000     1.167036   36.494000     0.998231   31.215334   \n",
       "4       0.000000     1.112305   36.494000     0.960844   31.524668   \n",
       "...          ...          ...         ...          ...         ...   \n",
       "1009  242.700000     1.015790  241.240000     0.948966  225.370000   \n",
       "1010  239.402500     1.003286  241.240000     0.937284  225.370000   \n",
       "1011  238.110000     1.026728  241.240000     0.959185  225.370000   \n",
       "1012  237.342425     1.031205  241.240000     0.963367  225.370000   \n",
       "1013  235.300000     1.061702  241.240000     0.991858  225.370000   \n",
       "\n",
       "           hpn1p  hpn1p_Ratio       lpn1p  lpn1p_Ratio  \n",
       "0      30.266666     1.055176   29.128000     1.015479  \n",
       "1      30.104000     1.019300   29.333332     0.993206  \n",
       "2      31.442000     1.044492   30.224000     1.004031  \n",
       "3      33.232666     1.062743   31.215334     0.998231  \n",
       "4      33.253334     1.013533   31.524668     0.960844  \n",
       "...          ...          ...         ...          ...  \n",
       "1009  241.240000     1.015790  235.300000     0.990779  \n",
       "1010  238.964600     0.993822  232.040000     0.965024  \n",
       "1011  235.500000     1.002298  231.290000     0.984380  \n",
       "1012  230.915000     0.987069  225.370000     0.963367  \n",
       "1013  230.915000     1.016262  225.370000     0.991858  \n",
       "\n",
       "[1014 rows x 26 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logarithmic detrending applied\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_39143/4169364875.py:316: RuntimeWarning: divide by zero encountered in log\n",
      "  df_array = np.log(df_array)\n",
      "/home/ubuntu/LTM_Bot/LTM-Long-short_Term_Memory.Bot-API_0.00/env/lib/python3.10/site-packages/simdkalman/primitives.py:23: RuntimeWarning: invalid value encountered in matmul\n",
      "  return np.matmul(A, B)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape:  (737, 10, 12)\n",
      "X_predict shape (1005, 10, 12)\n",
      "y_train shape:  1\n",
      "y_predict shape 1\n",
      "Input shape obtained is: (10, 12)\n"
     ]
    }
   ],
   "source": [
    "'''Function for making sequences (blocks) of test and train data'''\n",
    "def building_data_sequences(data_X,data_Y, timesteps): #timesteps means how many days we consider for each block\n",
    "\n",
    "    X=[]\n",
    "    y_MPNxP = []\n",
    "    for i in range(len(data_X)-timesteps+1):  #how it works: every timesteps (e.g. 10 days) a block is constituted and for each block data and true values are stored\n",
    "        X.append(data_X[i:(i+timesteps),:])\n",
    "        y_MPNxP.append(data_Y[i+timesteps-1])\n",
    "    return np.array(X), [np.array(y_MPNxP)]\n",
    "\n",
    "'''Function for computing the analytical parameters'''\n",
    "def sir_parameters(x,y): #sir stands for slope, intercept, rvalue (actually there's also the average trend line distance or avg_tld, but it came later)\n",
    "\n",
    "  analytical_params = linregress(x, y)\n",
    "  slope = analytical_params.slope\n",
    "  intercept = analytical_params.intercept\n",
    "  rvalue = analytical_params.rvalue #pay attention that here we have the correlaton coefficient (so not r2 that is the coefficient of determination)\n",
    "  x_trend_line = slope*x + intercept #this is computed just for the avg_tld\n",
    "  avg_trend_line_distance = np.mean(np.abs(x_trend_line-y))\n",
    "  return slope,intercept,rvalue**2,avg_trend_line_distance\n",
    "\n",
    "'''This is the timestep which indicates the window size'''\n",
    "model_case_version_time_steps= '10'\n",
    "'''This is the number of periods defined in the target(MPNxP) where x is the number of periods'''\n",
    "model_case_version_main_target_code='5'\n",
    "\n",
    "#today = '20220706'\n",
    "today = datetime.today().strftime('%Y%m%d') #just for names of files (for now)\n",
    "print('Today is', today)\n",
    "\n",
    "''' Here, we define the case name'''\n",
    "case = 'TSLA'\n",
    "print(case)\n",
    "\n",
    "'''Here, we define the list of targets we are going to work on and also the average for each target (this value is used during the training for normalization\\rescaling of some analytical parameters)'''\n",
    "targets =['MPN'+model_case_version_main_target_code+'P']  #this must be changed whenever tha targets change\n",
    "avg_prices_list = []\n",
    "\n",
    "#1L = dispersion = 1\n",
    "#4L = slope + intercept + resqr + dispersion = 1\n",
    "#5L = all the weighting = 1\n",
    "#new paramaters case = intercept + slope + end intercept + correlation + dispersion\n",
    "slope_weighting_exponent_ratio = 1\n",
    "intercept_weighting_exponent_ratio = 1\n",
    "end_intercept_weighting_exponent_ratio = 0\n",
    "rsqr_weighting_exponent_ratio = 1\n",
    "dispersion_weighting_exponent_ratio = 3\n",
    "\n",
    "slope_weighting_exponent_predicted_actual = 0\n",
    "intercept_weighting_exponent_predicted_actual = 0\n",
    "rsqr_weighting_exponent_predicted_actual = 0\n",
    "dispersion_weighting_exponent_predicted_actual = 0\n",
    "\n",
    "analytical_parametrs = str(intercept_weighting_exponent_ratio)+str(slope_weighting_exponent_ratio)+str(end_intercept_weighting_exponent_ratio)+str(rsqr_weighting_exponent_ratio)+str(dispersion_weighting_exponent_ratio)\n",
    "print(analytical_parametrs)\n",
    "\n",
    "''' Conection to PostgreSQL '''\n",
    "# The credentials to conect to the database\n",
    "hostname = 'database-1.ctzm0hf7fhri.eu-central-1.rds.amazonaws.com'\n",
    "database = 'dyDATA_new'\n",
    "username = 'postgres'\n",
    "pwd = 'Proc2023awsrdspostgresql'\n",
    "port_id = 5432\n",
    "conn = None\n",
    "\n",
    "#this helps to retreive the data for a particular asset from the database\n",
    "asset_script=\"SELECT * FROM \"+'\\\"'+\"ASSET_\"+case+'\\\"'+\".features_targets_input_view WHERE features_targets_input_view.\"+'\\\"'+\"cleaned_raw_features_environment_PK\"+'\\\"'+ \"= 4\"\n",
    "asset_script\n",
    "\n",
    "''' The active financial assets '''\n",
    "# Here we select the active financial asset from the financial asset list table\n",
    "try:\n",
    "  with psycopg2.connect(\n",
    "      host = hostname,\n",
    "      dbname = database,\n",
    "      user = username,\n",
    "      password = pwd,\n",
    "      port = port_id\n",
    "  ) as conn:\n",
    "\n",
    "    with conn.cursor(cursor_factory=psycopg2.extras.DictCursor) as cur:\n",
    "        print('You are connect to the Database:',database)\n",
    "        select_script = asset_script # do not forget to set to asset_script when data has been uploaded\n",
    "        cur.execute(select_script)\n",
    "        data = cur.fetchall()\n",
    "        cols = []\n",
    "        # loop to create the dataframe that contains the active financial assets\n",
    "        for rec in cur.description:\n",
    "            cols.append(rec[0])\n",
    "        dohlcav_mpnxp_data= pd.DataFrame(data = data, columns = cols)\n",
    "        print('Your SQL has executed successfully')\n",
    "\n",
    "except Exception as error:\n",
    "  print(error)\n",
    "\n",
    "finally:\n",
    "  if conn is not None:\n",
    "     conn.close()\n",
    "\n",
    "\n",
    "if case=='TSLA':\n",
    "  dohlcav_mpnxp_data = dohlcav_mpnxp_data.loc[dohlcav_mpnxp_data['cleaned_raw_features_DCP_date_current_period'] >= '2020-01-01'].reset_index(drop=True)\n",
    "\n",
    "\n",
    "filtered_columns_1=list(dohlcav_mpnxp_data.columns[:9])#to filter out the dates columns and features columns\n",
    "filtered_columns_2=[x for x in dohlcav_mpnxp_data.columns if  targets[0][3:] in x ]#feature out the main target columns\n",
    "#special condition for filtering if the main target is MPN1P\n",
    "if model_case_version_main_target_code=='1':\n",
    "  temp=filtered_columns_2[0]\n",
    "  temp_2=filtered_columns_2[1]\n",
    "  filtered_columns_2[0]=filtered_columns_2[2]\n",
    "  filtered_columns_2[1]=temp\n",
    "  filtered_columns_2[2]=temp_2\n",
    "\n",
    "#to add the last two constant columns to the table\n",
    "filtered_columns_3=['calculated_targets_HPN1P','calculated_targets_LPN1P']\n",
    "filtered_columns=filtered_columns_1+filtered_columns_2+filtered_columns_3\n",
    "\n",
    "print(filtered_columns)\n",
    "\n",
    "dohlcav_mpnxp_data=dohlcav_mpnxp_data[filtered_columns]\n",
    "\n",
    "if model_case_version_main_target_code=='1':\n",
    "  dohlcav_mpnxp_data.columns=[\"ID\",\"DCP_date_current_period\",\"DNCP_day_number_current_period\",\"OPCP_open_price_current_period\",\"HPCP_high_price_current_period\",\"LPCP_low_price_current_period\"\n",
    ",\"CPCP_close_price_current_period\",\"ACPCP_adjusted_close_price_current_period\",\"VTCP_volume_of_transactions_current_period\",\"MPN\"+model_case_version_main_target_code+\"P_median_price_next_\"+model_case_version_main_target_code+\"_periods\",\"HPN\"+model_case_version_main_target_code+\"P_highest_price_next_\"+model_case_version_main_target_code+\"_periods\",\"LPN\"+model_case_version_main_target_code+\"P_lowest_price_next_\"+model_case_version_main_target_code+\"_periods\",\"HPN1P_high_price_next_1_period\",\n",
    "\"LPN1P_low_price_next_1_period\"\n",
    "]\n",
    "else:\n",
    "  dohlcav_mpnxp_data = dohlcav_mpnxp_data.rename(columns={\"cleaned_raw_features_id\":\"ID\",\n",
    "                                \"cleaned_raw_features_DCP_date_current_period\": \"DCP_date_current_period\",\n",
    "                                \"calculated_features_DNCP\":\"DNCP_day_number_current_period\",\n",
    "                                \"cleaned_raw_features_OPCP_open_price_current_period\":\"OPCP_open_price_current_period\",\n",
    "                                \"cleaned_raw_features_HPCP_high_price_current_period\":\"HPCP_high_price_current_period\",\n",
    "                                \"cleaned_raw_features_LPCP_low_price_current_period\":\"LPCP_low_price_current_period\",\n",
    "                                \"cleaned_raw_features_CPCP_close_price_current_period\": \"CPCP_close_price_current_period\",\n",
    "                                \"cleaned_raw_features_ACPCP_adjusted_close_price_current_period\":\"ACPCP_adjusted_close_price_current_period\",\n",
    "                                \"cleaned_raw_features_VTCP_volume_of_transactions_current_period\":\"VTCP_volume_of_transactions_current_period\",\n",
    "                                filtered_columns_2[0]:\"MPN\"+model_case_version_main_target_code+\"P_median_price_next_\"+model_case_version_main_target_code+\"_periods\",\n",
    "                                filtered_columns_2[1]:\"HPN\"+model_case_version_main_target_code+\"P_highest_price_next_\"+model_case_version_main_target_code+\"_periods\",\n",
    "                                filtered_columns_2[2]:\"LPN\"+model_case_version_main_target_code+\"P_lowest_price_next_\"+model_case_version_main_target_code+\"_periods\",\n",
    "                                filtered_columns_3[0]:\"HPN1P_high_price_next_1_period\",\n",
    "                                filtered_columns_3[1]:\"LPN1P_low_price_next_1_period\",\n",
    "                                })\n",
    "\n",
    "# dohlcav_mpnxp_data = dohlcav_mpnxp_data.replace(',','', regex=True) #remove the ',' otherwise it's impossible to deal with numbers in the dataset\n",
    "dohlcav_mpnxp_data.tail(int(model_case_version_main_target_code)+1) # to visualize likely columns with NaN values in the dataset\n",
    "''' This comprises the list of target in our datasets'''\n",
    "targets_list=[\"MPN\"+model_case_version_main_target_code+\"P_median_price_next_\"+model_case_version_main_target_code+\"_periods\",\n",
    "                        \"HPN\"+model_case_version_main_target_code+\"P_highest_price_next_\"+model_case_version_main_target_code+\"_periods\",\n",
    "                        \"LPN\"+model_case_version_main_target_code+\"P_lowest_price_next_\"+model_case_version_main_target_code+\"_periods\",\n",
    "                        'HPN1P_high_price_next_1_period','LPN1P_low_price_next_1_period']\n",
    "\n",
    "for i in targets_list:\n",
    "  if targets[0] in i:\n",
    "    main_target_column=i\n",
    "\n",
    "'''This function helps to locate the index of where the Nan Value begins in our target(MPNxP) and later used for the stop actual variable in configuration parameter section'''\n",
    "'''function is now redundant since the FDU does this work now'''\n",
    "def stop_target_value(x):\n",
    "  for i in x:\n",
    "    if targets[0] in i:\n",
    "       temp=dohlcav_mpnxp_data[i].apply(lambda y: math.isnan(float(y)) or y=='#NUM!')\n",
    "       for i in range(len(temp)):\n",
    "          if temp[i]== True:\n",
    "             stop=i\n",
    "             return stop\n",
    "# if dohlcav_mpnxp_data[main_target_column].isnull().values.any():\n",
    "#      stop_target=stop_target_value(targets_list)\n",
    "# else:\n",
    "''' What is stop target?'''\n",
    "stop_target=dohlcav_mpnxp_data.index[-1]+1\n",
    "\n",
    "''' Here what we do is to separate data in two parts: the first goes from the beginnig of 2020 to the end of 2021 and\n",
    "it is used for training the model; the second goes from the beginning of 2022 until the end\n",
    "(it dependes on the last update we did for the dataset) and it is used for testing the model.\n",
    "To do that, we siply compute the size of the training set and then we use this value (later in the code) to split the dataset '''\n",
    "\n",
    "dohlcav_mpnxp_data['DCP_date_current_period'] = pd.to_datetime(dohlcav_mpnxp_data['DCP_date_current_period']) #date values in the dataset are converted\n",
    "\n",
    "start_date = pd.Timestamp(str(dohlcav_mpnxp_data['DCP_date_current_period'].iloc[0])) #start date of the training set\n",
    "# filtered_data = dohlcav_mpnxp_data[dohlcav_mpnxp_data['DCP_date_current_period'] == '2008-01-02']\n",
    "# start_date = filtered_data['DCP_date_current_period'].iloc[0]\n",
    "temp_train_end_date = pd.Timestamp('2022-12-30')#this specifies the training end date\n",
    "idx=dohlcav_mpnxp_data.index[dohlcav_mpnxp_data['DCP_date_current_period']==temp_train_end_date].values[0]\n",
    "new_idx=idx-int(model_case_version_main_target_code)\n",
    "\n",
    "''' why to avoid tail values'''\n",
    "train_end_date=dohlcav_mpnxp_data.loc[new_idx,'DCP_date_current_period']#we move the training date in such a way that we avoid the tail values\n",
    "prediction_end_date=pd.Timestamp(str(dohlcav_mpnxp_data['DCP_date_current_period'].iloc[-1]))\n",
    "\n",
    "'''This is the mask of booleans that will be used to filter data and take just what we need (data from the beginning until the end of 2021)'''\n",
    "\n",
    "train_mask = (dohlcav_mpnxp_data['DCP_date_current_period'] <= train_end_date)#to select data for training\n",
    "prediction_mask = (dohlcav_mpnxp_data['DCP_date_current_period'] <= prediction_end_date)#to select data for prediction\n",
    "\n",
    "training_size = dohlcav_mpnxp_data.loc[train_mask].shape[0] #the mask is applied and from the correspondent dataframe we take just the shape[0] (the size\\the number of rows)\n",
    "prediction_size = dohlcav_mpnxp_data.loc[prediction_mask].shape[0]\n",
    "print('Training size: ', training_size)\n",
    "print('Prediction size: ', prediction_size)\n",
    "#print('Test size: ', dohlcav_mpnxp_data.shape[0] - training_size)#this is to define our testing size but this is commented becausing testing percent is 0\n",
    "\n",
    "'''These will be used in the predictions output file (in order to know from which point starting to paste the results).'''\n",
    "dates = dohlcav_mpnxp_data.iloc[int(model_case_version_time_steps)-1:,1].apply(lambda x: x.date().strftime('%Y-%m-%d')).reset_index(drop=True)\n",
    "dates\n",
    "\n",
    "'''Building the dataframe with just the necessary columns (removing 'id', 'uuid' and 'ACPCP_adjusted_close_price_current_period' column)'''\n",
    "\n",
    "#pay attention here because everytime targets change, also the name of the columns change\n",
    "df = dohlcav_mpnxp_data.drop([\"ID\",\"DCP_date_current_period\"], axis=1)\n",
    "\n",
    "'''Renaming columns to have a more compact and a better reading of the df'''\n",
    "#pay attention here because everytime targets change, also the name of the columns change\n",
    "df = df.rename(columns={\"DNCP_day_number_current_period\": \"DNCP\",\n",
    "                        \"OPCP_open_price_current_period\":\"OPCP\",\n",
    "                        \"HPCP_high_price_current_period\":\"HPCP\",\n",
    "                        \"LPCP_low_price_current_period\":\"LPCP\",\n",
    "                        \"CPCP_close_price_current_period\":\"CPCP\",\n",
    "                        \"ACPCP_adjusted_close_price_current_period\": \"ACPCP\",\n",
    "                        \"VTCP_volume_of_transactions_current_period\":\"VTCP\",\n",
    "                        \"MPN\"+model_case_version_main_target_code+\"P_median_price_next_\"+model_case_version_main_target_code+\"_periods\":\"MPN\"+model_case_version_main_target_code+\"P\",\n",
    "                        \"HPN\"+model_case_version_main_target_code+\"P_highest_price_next_\"+model_case_version_main_target_code+\"_periods\":\"HPN\"+model_case_version_main_target_code+\"P\",\n",
    "                        \"LPN\"+model_case_version_main_target_code+\"P_lowest_price_next_\"+model_case_version_main_target_code+\"_periods\":\"LPN\"+model_case_version_main_target_code+\"P\",\n",
    "                        'HPN1P_high_price_next_1_period':'hpn1p',\n",
    "                        'LPN1P_low_price_next_1_period':'lpn1p'})\n",
    "\n",
    "#df.columns = ['DNCP', 'OPCP', 'HPCP', 'LPCP', 'CPCP', 'ACPCP', 'VTCP', 'MPN1P', 'HPN1P', 'LPN1P', 'HPN1P', 'LPN1P']\n",
    "base_target_code = 'MPN' + model_case_version_main_target_code + 'P'\n",
    "base_target_column_index = df.columns.get_loc(base_target_code)\n",
    "\n",
    "def new_target_column(target_code , shift_back_period):\n",
    "  prev_target = df[target_code]\n",
    "  new_target = prev_target[:-shift_back_period]\n",
    "  first_dates_handling = [0] * shift_back_period\n",
    "  new_target=np.concatenate((first_dates_handling,new_target))\n",
    "  return new_target\n",
    "\n",
    "#Adding multiple targets\n",
    "new_target_index = base_target_column_index\n",
    "for i in range(int(model_case_version_main_target_code)):\n",
    "  new_target_code = 'MPN-' + str(i+1) + 'P'\n",
    "  df.insert(new_target_index+1,new_target_code,new_target_column(base_target_code,i+1))\n",
    "  new_target_index = new_target_index + 1\n",
    "  targets.append(new_target_code)\n",
    "\n",
    "\n",
    "# '''Ratio Transformation for features'''\n",
    "df.insert(7,'OPCP_Ratio',df['OPCP']/df['CPCP'])\n",
    "df.insert(8,'HPCP_Ratio',df['HPCP']/df['CPCP'])\n",
    "df.insert(9,'LPCP_Ratio',df['LPCP']/df['CPCP'])\n",
    "df.insert(10,'ACPCP_Ratio',df['ACPCP']/df['CPCP'])\n",
    "df.insert(df.columns.get_loc('MPN'+model_case_version_main_target_code+'P') ,'MPN'+ model_case_version_main_target_code +'P_Ratio',df['MPN'+ model_case_version_main_target_code +'P'].shift(5)/df['CPCP'].shift(5))\n",
    "df.insert(df.columns.get_loc('HPN'+model_case_version_main_target_code+'P'),'HPN'+ model_case_version_main_target_code +'P_Ratio',df['HPN'+ model_case_version_main_target_code +'P']/df['CPCP'])\n",
    "df.insert(df.columns.get_loc('LPN'+model_case_version_main_target_code+'P'),'LPN'+ model_case_version_main_target_code +'P_Ratio',df['LPN'+ model_case_version_main_target_code +'P']/df['CPCP'])\n",
    "df.insert(df.columns.get_loc('hpn1p')+1,'hpn1p_Ratio',df['hpn1p']/df['CPCP'])\n",
    "df.insert(df.columns.get_loc('lpn1p')+1,'lpn1p_Ratio',df['lpn1p']/df['CPCP'])\n",
    "\n",
    "\n",
    "for target in targets:\n",
    "  avg_prices_list.append(df[target].astype(float).mean())\n",
    "\n",
    "main_target_code_integer = int(model_case_version_main_target_code)\n",
    "\n",
    "#this is calculate the period day number to be used in the training section\n",
    "dncp_train = dohlcav_mpnxp_data[train_mask]['DNCP_day_number_current_period'].replace(',','', regex=True)[int(model_case_version_time_steps)-1+main_target_code_integer:]\n",
    "dncp_train= dncp_train.astype(int).to_numpy()\n",
    "span_dncp_train=dncp_train[-1] - dncp_train[0] + 1\n",
    "positions_day_number_train= dncp_train-dncp_train[0]+1\n",
    "\n",
    "#this is calculate the period day number to be used in the prediction section\n",
    "dncp = dohlcav_mpnxp_data['DNCP_day_number_current_period'].replace(',','', regex=True)[int(model_case_version_time_steps)-1+main_target_code_integer:]\n",
    "dncp = dncp.astype(int).to_numpy()\n",
    "span_dncp=dncp[-1] - dncp[0] + 1\n",
    "positions_day_number = dncp-dncp[0]+1\n",
    "\n",
    "'''The padding point value is calculated for computing the value at the end of the trend line. We'll see better during the training and the application of vertical padding '''\n",
    "padding_point = positions_day_number[0]\n",
    "\n",
    "display(df)\n",
    "\n",
    "#seperate training dataset and apply pre-treament only on the training dataset\n",
    "train_dataset=df.iloc[main_target_code_integer:new_idx+1,:]\n",
    "\n",
    "train_df_array=train_dataset.to_numpy(dtype='float64')\n",
    "\n",
    "#application of logarithmic detrending\n",
    "if not (train_df_array < 0).any():\n",
    "    print('logarithmic detrending applied')\n",
    "    train_df_array = np.log(train_df_array)\n",
    "\n",
    "kalman_train=True#not applying kalman Filter\n",
    "'''Preprocessing consists, in this case, to transform the dataset through 3 filters: Kalman, E_logrithmic and RobustScaler (in this order)'''\n",
    "if kalman_train: #necessary to deal with filter results\n",
    "  '''Application of the Kalman filter (rounding data)'''\n",
    "  kalman_smoother=tsmoothie.KalmanSmoother(component='level_trend',  component_noise={'level':0.1, 'trend':0.1}) #values for Kalman filters parameters\n",
    "                                                                                                                  #are taken from an example in the original code\n",
    "                                                                                                                  #of the library\n",
    "  temp_df=pd.DataFrame(train_df_array,columns=train_dataset.columns)\n",
    "  for i in range(len(temp_df.columns)): #this is the more convenient way I've found to apply the filter\n",
    "    kalman_smoother.smooth(temp_df[temp_df.columns[i]])\n",
    "    train_df_array[:,i] = kalman_smoother.smooth_data\n",
    "\n",
    "#fitting the robust scaler onn both the features and target column separately for the training dataset\n",
    "train_robust_scaler_features= RobustScaler().fit(train_df_array[:,:12])\n",
    "train_robust_scaler_target=RobustScaler().fit(train_df_array[:,12:12+int(model_case_version_main_target_code)+1])\n",
    "\n",
    "#performing transform on the features\n",
    "train_df_features=train_robust_scaler_features.transform(train_df_array[:,:12])\n",
    "#performing transform on the target\n",
    "train_df_target=train_robust_scaler_target.transform(train_df_array[:,12:12+int(model_case_version_main_target_code)+1])\n",
    "\n",
    "df_array=df.to_numpy(dtype='float64')\n",
    "\n",
    "# df_array = df_array[5:]\n",
    "#applying logarithmic detrending to the prediction/full dataset\n",
    "if not (df_array < 0).any():\n",
    "    df_array = np.log(df_array)\n",
    "\n",
    "kalman_predict=True\n",
    "if kalman_predict:\n",
    "  '''Preprocessing consists, in this case, to transform the dataset through 3 filters: Kalman, E_logrithmic and RobustScaler (in this order)'''\n",
    "\n",
    "  '''Application of the Kalman filter (rounding data)'''\n",
    "  kalman_smoother=tsmoothie.KalmanSmoother(component='level_trend',  component_noise={'level':0.1, 'trend':0.1}) #values for Kalman filters parameters\n",
    "                                                                                                                  #are taken from an example in the original code\n",
    "                                                                                                                  #of the library\n",
    "  temp_df=pd.DataFrame(df_array,columns=df.columns)\n",
    "  for i in range(len(df.columns)): #this is the more convenient way I've found to apply the filter\n",
    "    kalman_smoother.smooth(temp_df[temp_df.columns[i]])\n",
    "    df_array[:,i] = kalman_smoother.smooth_data\n",
    "\n",
    "#robust scaler transform on the features\n",
    "prediction_df_features=train_robust_scaler_features.transform(df_array[:,:12])\n",
    "\n",
    "#robust scaler transform on the target\n",
    "prediction_df_targets=train_robust_scaler_target.transform(df_array[:,12:12+int(model_case_version_main_target_code)+1])\n",
    "\n",
    "'''Creating the input blocks for the models. The timestep value must be changed according to the targets and dataset we are working with'''\n",
    "X_train, y_train =building_data_sequences(train_df_features,train_df_target,timesteps=int(model_case_version_time_steps))\n",
    "X_predict,y_predict=building_data_sequences(prediction_df_features,prediction_df_targets,timesteps=int(model_case_version_time_steps))#see Functions section above;\n",
    "#X_test, y_test = building_data_sequences(test_data, timesteps=int(model_case_version_time_steps))\n",
    "\n",
    "print('X_train shape: ',X_train.shape)\n",
    "print('X_predict shape',X_predict.shape)\n",
    "print('y_train shape: ',len(y_train))\n",
    "print('y_predict shape',len(y_predict))\n",
    "\n",
    "'''In this section, we're going to define some variables that will be useful during the training and testing of the model'''\n",
    "input_shape=((X_train).shape[1],(X_train).shape[2])\n",
    "print(\"Input shape obtained is:\",input_shape)\n",
    "'''We need actual values (true values) to compute some analytical parameters during the training'''\n",
    "df_actual = df.iloc[int(model_case_version_time_steps)-1:,:].reset_index(drop=True)\n",
    "\n",
    "actuals_cols = [] #simply put the targets columns into a list (in order to be iterated during training)\n",
    "\n",
    "for target in targets:\n",
    "  actuals_cols.append(np.array(df_actual[target].astype(float)))\n",
    "\n",
    "'''this indicates the index from which we start to replace the actual target with the predicted target to be used in the prediction section'''\n",
    "stop_actual=stop_target-int(model_case_version_time_steps)+1-main_target_code_integer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "ROOT = Path('/home/ubuntu/LTM_Bot/LTM-Long-short_Term_Memory.Bot-API_0.00')\n",
    "DATA = ROOT / 'data'\n",
    "MODELS = ROOT / 'models'\n",
    "\n",
    "'''Model parameters: in order to understand, consult the original documentation (case_version_cat Tab) '''\n",
    "timesteps = 20\n",
    "n_epochs = 100\n",
    "batch = 64\n",
    "\n",
    "'''These are the exponent used to define the number of nodes for each layer'''\n",
    "twoexp_nodes_number_layer_1 = 7\n",
    "twoexp_nodes_number_layer_2 = 10\n",
    "twoexp_nodes_number_layer_3 = 7\n",
    "twoexp_nodes_number_layer_4 = 6\n",
    "twoexp_nodes_number_layer_5 = 0\n",
    "\n",
    "lr=0.0005 #learning rate\n",
    "\n",
    "max_iterations =1#maximum number of iterations for the while loop (we will ee later in the code)\n",
    "precision = 0.00000000001 #this precision is related to the quality of the compound_run_term value we want to obtain (that is representative of the quality of the model)\n",
    "attenuation_factor = 0.75 #it us used in the computation of the attenuated_padding_value (see custom_loss_function)\n",
    "attenuated_padding_value = 1\n",
    "model_case_version_main_target_code = '5'\n",
    "\n",
    "plot_target = False\n",
    "plot_loss = True\n",
    "TRAIN = False"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
